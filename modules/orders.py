# modules/orders.py
# Obs≈Çuga zam√≥wie≈Ñ: pliki + rƒôczne wpisy, z op√≥≈∫nionym przetwarzaniem

import streamlit as st
import pandas as pd
import numpy as np
import traceback
import sys
import re
from modules.ui_strings import STR

# Cache na zam√≥wienia z plik√≥w
if "orders_cache" not in st.session_state:
    st.session_state["orders_cache"] = {
        "files_keys": None,      # identyfikatory plik√≥w
        "orders_all": None,
        "orders_agg": None,
    }



# ===== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–∫–∞–∑–æ–≤ =====

# –Ø–∫–æ—Ä–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª–∞ (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä, –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å)
KNOWN_ARTS_SET = {
    "1",
    "2",
    "21",
    "22",
    "61",
    "MH-1875",
    "MN 5029",
    "MH-9036",
    "DAF H-PALETTEN",
    "8309024074",
    "8309023044",
    "0004 MAN",
    "MH-1872",
    "8309021164",
}

# –í–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ —Å –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏
ARTICLE_HEADER_CANDIDATES = [
    "NR MATERIALU",
    "NR MATERIAU",
    "MATERIALNUMMER",
    "ARTIKELNR",
    "ARTIKEL",
]

def _looks_like_article(value: str) -> bool:
    """
    –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, –ø–æ—Ö–æ–∂–µ –ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ –∞—Ä—Ç–∏–∫—É–ª:
    - –Ω–µ –ø—É—Å—Ç–æ
    - –Ω–µ —á–∏—Å—Ç–æ–µ '0'
    - —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –ø—Ä–æ–±–µ–ª—ã, —Ç–∏—Ä–µ.
    """
    v = str(value).strip()
    if not v:
        return False
    if v == "0":
        return False
    import re
    return bool(re.match(r"^[A-Za-z0-9\- ]+$", v))


def detect_order_structure(df_o):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å:
      - –∏–Ω–¥–µ–∫—Å –∫–æ–ª–æ–Ω–∫–∏ —Å –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏ (art_col)
      - –∏–Ω–¥–µ–∫—Å —Å—Ç—Ä–æ–∫–∏, —Å –∫–æ—Ç–æ—Ä–æ–π –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ (data_start_row)

    –õ–æ–≥–∏–∫–∞:
      1) –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Å—Ç—Ä–æ–∫—É —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏, –≥–¥–µ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ ARTICLE_HEADER_CANDIDATES.
      2) –ï—Å–ª–∏ –Ω–∞—à–ª–∏ ‚Äî art_col = —ç—Ç–∞ –∫–æ–ª–æ–Ω–∫–∞, data_start_row = —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–æ–∫–∞.
      3) –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –∏—â–µ–º –∫–æ–ª–æ–Ω–∫—É, –≥–¥–µ:
           - –º–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Ö–æ–∂–∏ –Ω–∞ –∞—Ä—Ç–∏–∫—É–ª—ã,
           - –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª–∞ –∏–∑ KNOWN_ARTS_SET.
         –í –∫–∞—á–µ—Å—Ç–≤–µ data_start_row –±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É, –≥–¥–µ –ø–æ—è–≤–ª—è–µ—Ç—Å—è —á—Ç–æ-—Ç–æ, –ø–æ—Ö–æ–∂–µ–µ –Ω–∞ –∞—Ä—Ç–∏–∫—É–ª.
    """
    max_rows_to_check = min(200, df_o.shape[0])  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º—Å—è –≤–µ—Ä—Ö–Ω–µ–π —á–∞—Å—Ç—å—é —Ç–∞–±–ª–∏—Ü—ã

    # --- –®–∞–≥ 1: –ø–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø–æ ARTICLE_HEADER_CANDIDATES ---
    art_col_by_header = None
    header_row_idx = None

    for row_idx in range(max_rows_to_check):
        row_vals = df_o.iloc[row_idx, :]
        for col_idx, cell in enumerate(row_vals):
            text = str(cell).strip().upper()
            if text in ARTICLE_HEADER_CANDIDATES:
                art_col_by_header = col_idx
                header_row_idx = row_idx
                break
        if art_col_by_header is not None:
            break

    if art_col_by_header is not None:
        # –ù–∞—à–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–æ–ª–æ–Ω–∫–∏ –∞—Ä—Ç–∏–∫—É–ª–∞
        art_col = art_col_by_header
        data_start_row = header_row_idx + 1
        return {
            "art_col": art_col,
            "data_start_row": data_start_row,
        }

    # --- –®–∞–≥ 2: –±–µ–∑ —è–≤–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ ‚Äî –∏—â–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É ---
    best_col = None
    best_score = -1
    best_first_row = None

    n_cols = df_o.shape[1]

    for col_idx in range(n_cols):
        col = df_o.iloc[:max_rows_to_check, col_idx]

        known_hits = 0
        article_like = 0
        first_article_row = None

        for row_idx, val in col.items():
            v = str(val).strip()
            if not v:
                continue

            v_upper = v.upper()

            # –Ø–∫–æ—Ä–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª–∞
            if v_upper in KNOWN_ARTS_SET:
                known_hits += 1
                if first_article_row is None:
                    first_article_row = row_idx

            # –ü–æ—Ö–æ–∂–µ –Ω–∞ –∞—Ä—Ç–∏–∫—É–ª
            if _looks_like_article(v):
                article_like += 1
                if first_article_row is None:
                    first_article_row = row_idx

        # –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–æ–Ω–∫–∏:
        #  - —Å–Ω–∞—á–∞–ª–∞ –≤–∞–∂–Ω—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å KNOWN_ARTS_SET
        #  - –∑–∞—Ç–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ "–ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ –∞—Ä—Ç–∏–∫—É–ª" –∑–Ω–∞—á–µ–Ω–∏–π
        score = known_hits * 10 + article_like

        if score > best_score and article_like > 0:
            best_score = score
            best_col = col_idx
            best_first_row = first_article_row

    if best_col is None:
        # –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –≤–µ—Ä–Ω—ë–º –¥–µ—Ñ–æ–ª—Ç, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É
        return {
            "art_col": 0,
            "data_start_row": 2,  # –∫–∞–∫ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ
        }

    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–æ–ª–æ–Ω–∫—É –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
    art_col = best_col
    # –ù–∞—á–∞–ª–æ–º –¥–∞–Ω–Ω—ã—Ö —Å—á–∏—Ç–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É, –≥–¥–µ –≤—Å—Ç—Ä–µ—Ç–∏–ª—Å—è –∞—Ä—Ç–∏–∫—É–ª
    data_start_row = best_first_row if best_first_row is not None else 2

    return {
        "art_col": art_col,
        "data_start_row": data_start_row,
    }




# ---------- Parsowanie pojedynczego pliku zam√≥wie≈Ñ ----------

def parse_order_file_to_df(fobj):
    """
    Czyta pojedynczy plik zam√≥wie≈Ñ (XLSX w formacie z OrderMasterSheet)
    BEZ u≈ºycia pandas.read_excel / openpyxl, ≈ºeby uniknƒÖƒá b≈Çƒôdu wildcard.

    Oczekiwana struktura arkusza OrderMasterSheet:
    - kolumna A: Materialnummer / Nr materiau (ARTIKELNR)
    - kolumna B: Artikelgesamtmenge / Ilo≈õƒá sztuk (ca≈Çkowita ilo≈õƒá)
    - kolumna C: liczba palet (brak nag≈Ç√≥wka)
    - kolumna D: szt./wiƒÖzka
    Reszta kolumn ignorowana.

    Zwraca DataFrame z kolumnami:
      ARTIKELNR (upper), ORDER_PALLETS (int), ORDER_QTY (float)
    """
    import io
    import zipfile
    import xml.etree.ElementTree as ET

    name = getattr(fobj, "name", "uploaded")
    df_o = None

    # CSV / TXT ‚Äì –Ω–∞ –±—É–¥—É—â–µ–µ
    if name.lower().endswith((".csv", ".txt")):
        fobj.seek(0)
        try:
            df_o = pd.read_csv(
                fobj,
                sep=";",
                dtype=str,
                encoding="utf-8",
                header=None,
            )
        except Exception as e:
            print("\n===== ORDER PARSE ERROR (CSV/TXT) =====", file=sys.stderr)
            traceback.print_exc()
            print("===== END ORDER PARSE ERROR =====\n", file=sys.stderr)
            st.error(f"B≈ÇƒÖd czytania pliku zam√≥wienia {name}: {e}")
            return None

    # ---- XLSX: –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —á—Ç–µ–Ω–∏–µ XML ----
    else:
        try:
            fobj.seek(0)
            file_bytes = fobj.read()
            fobj.seek(0)

            with zipfile.ZipFile(io.BytesIO(file_bytes), "r") as zf:
                # workbook.xml ‚Äì szukamy arkusza z zam√≥wieniem
                with zf.open("xl/workbook.xml") as wb:
                    wb_tree = ET.parse(wb)
                    wb_root = wb_tree.getroot()
                ns = {"ns": "http://schemas.openxmlformats.org/spreadsheetml/2006/main"}

                sheet_id = None
                for sheet in wb_root.findall("ns:sheets/ns:sheet", ns):
                    sheet_name = sheet.attrib.get("name", "")
                    r_id = sheet.attrib.get("{http://schemas.openxmlformats.org/officeDocument/2006/relationships}id")
                    if sheet_name in ("OrderMasterSheet", "Order_Master_Sheet"):
                        sheet_id = r_id
                        break
                if sheet_id is None:
                    first_sheet = wb_root.find("ns:sheets/ns:sheet", ns)
                    if first_sheet is None:
                        raise ValueError("Brak arkuszy w pliku XLSX")
                    sheet_id = first_sheet.attrib.get(
                        "{http://schemas.openxmlformats.org/officeDocument/2006/relationships}id"
                    )

                # workbook.xml.rels ‚Äì ≈õcie≈ºka do pliku arkusza
                with zf.open("xl/_rels/workbook.xml.rels") as rels:
                    rels_tree = ET.parse(rels)
                    rels_root = rels_tree.getroot()
                rel_ns = {"rel": "http://schemas.openxmlformats.org/package/2006/relationships"}

                sheet_path = None
                for rel in rels_root.findall("rel:Relationship", rel_ns):
                    if rel.attrib.get("Id") == sheet_id:
                        sheet_path = rel.attrib.get("Target")
                        break
                if sheet_path is None:
                    raise ValueError("Nie mo≈ºna znale≈∫ƒá arkusza dla zam√≥wie≈Ñ (rels).")

                if not sheet_path.startswith("xl/"):
                    sheet_path = "xl/" + sheet_path

                # XML wybranego arkusza
                with zf.open(sheet_path) as sf:
                    sheet_tree = ET.parse(sf)
                    sheet_root = sheet_tree.getroot()

                # sharedStrings ‚Äì teksty
                shared_strings = []
                if "xl/sharedStrings.xml" in zf.namelist():
                    with zf.open("xl/sharedStrings.xml") as ssf:
                        ss_tree = ET.parse(ssf)
                        ss_root = ss_tree.getroot()
                    for si in ss_root.findall("{http://schemas.openxmlformats.org/spreadsheetml/2006/main}si"):
                        t = si.find("{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t")
                        shared_strings.append(t.text if t is not None else "")

                # wiersze + kom√≥rki
                rows_data = []
                for row_elem in sheet_root.findall(
                    ".//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}row"
                ):
                    row_values = []
                    last_col_idx = -1
                    for cell in row_elem.findall(
                        "{http://schemas.openxmlformats.org/spreadsheetml/2006/main}c"
                    ):
                        cell_ref = cell.attrib.get("r", "")
                        col_letters = "".join(ch for ch in cell_ref if ch.isalpha())
                        col_idx = 0
                        for ch in col_letters:
                            col_idx = col_idx * 26 + (ord(ch.upper()) - ord("A") + 1)
                        col_idx -= 1  # 0-based

                        while last_col_idx + 1 < col_idx:
                            row_values.append("")
                            last_col_idx += 1

                        v = cell.find(
                            "{http://schemas.openxmlformats.org/spreadsheetml/2006/main}v"
                        )
                        cell_type = cell.attrib.get("t")
                        if v is not None and v.text is not None:
                            if cell_type == "s":
                                idx = int(v.text)
                                value = shared_strings[idx] if 0 <= idx < len(shared_strings) else ""
                            else:
                                value = v.text
                        else:
                            value = ""

                        row_values.append(str(value))
                        last_col_idx = col_idx

                    if row_values:
                        rows_data.append(row_values)

                if not rows_data:
                    raise ValueError("Brak danych w arkuszu zam√≥wie≈Ñ.")

                max_cols = max(len(r) for r in rows_data)
                rows_padded = [r + [""] * (max_cols - len(r)) for r in rows_data]
                df_o = pd.DataFrame(rows_padded)

        except Exception as e:
            print("\n===== ORDER PARSE ERROR (XLSX ZIP/XML) =====", file=sys.stderr)
            traceback.print_exc()
            print("===== END ORDER PARSE ERROR =====\n", file=sys.stderr)
            st.error(f"B≈ÇƒÖd czytania pliku zam√≥wienia {name}: {e}")
            return None

    # ==== –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –ø–æ art_col –∏ data_start_row ====
    if df_o.shape[1] < 1:
        st.error(f"Plik {name} ma za ma≈Ço kolumn (oczekiwane >= 1).")
        return None

    structure = detect_order_structure(df_o)
    art_col = structure["art_col"]
    data_start_row = structure["data_start_row"]

    # –°–µ–∫—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö: –≤—Å—ë, —á—Ç–æ –Ω–∏–∂–µ data_start_row
    df_data = df_o.iloc[data_start_row:, :].copy()

    # –í–´–¢–ê–°–ö–ò–í–ê–ï–ú –∫–æ–ª–æ–Ω–∫—É –∞—Ä—Ç–∏–∫—É–ª–∞
    artikel_col = df_data.iloc[:, art_col].astype(str)

    # –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –∫–æ–ª–æ–Ω–æ–∫ —Å–ø—Ä–∞–≤–∞ –æ—Ç –∞—Ä—Ç–∏–∫—É≈Ça:
    # —Å–º–æ—Ç—Ä–∏–º –º–∞–∫—Å–∏–º—É–º 5 –∫–æ–ª–æ–Ω–æ–∫ –∏ —Å—Ä–∞–∑—É –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º —è–≤–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ.
    right_cols_indices = []
    max_right_span = 5

    for offset in range(1, max_right_span + 1):
        idx = art_col + offset
        if idx >= df_data.shape[1]:
            break

        col_raw = df_data.iloc[:, idx].astype(str)
        # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª—É
        col_num = pd.to_numeric(col_raw.str.replace(",", "."), errors="coerce")
        non_null = col_num.dropna()

        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–æ–Ω–∫—É —á–∏—Å–ª–æ–≤–æ–π, –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 1 —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.
        # –í–∞–∂–Ω–æ –¥–ª—è —Ñ–∞–π–ª–æ–≤ –¥–æ–∑–∞–∫–∞–∑–æ–≤ (dom√≥wienia), –≥–¥–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –≤—Å–µ–≥–æ 1 —Å—Ç—Ä–æ–∫–∞ –∏–∑ 300.
        if len(non_null) < 1:
            continue

        right_cols_indices.append(idx)

    # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–π –ø–æ–¥—Ö–æ–¥—è—â–µ–π —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å–ø—Ä–∞–≤–∞ ‚Äì –¥–∞–ª—å—à–µ —Å–º—ã—Å–ª–∞ –Ω–µ—Ç
    if not right_cols_indices:
        st.error(f"Plik {name}: brak liczbowych kolumn z ilo≈õciami po kolumnie artyku≈Çu.")
        return None


    # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å–ø—Ä–∞–≤–∞ ‚Äì –¥–∞–ª—å—à–µ —Å–º—ã—Å–ª–∞ –Ω–µ—Ç
    if not right_cols_indices:
        st.error(f"Plik {name}: brak kolumn z ilo≈õciami po kolumnie artyku≈Çu.")
        return None

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞: –±–µ—Ä—ë–º –ø–æ–¥—Ç–∞–±–ª–∏—Ü—É —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ —Å–ø—Ä–∞–≤–∞
    right_part = df_data.iloc[:, right_cols_indices].copy()

    # –ü–æ–ø—Ä–æ–±—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –≥—Ä—É–±–æ:
    # - PALLETS: —Ü–µ–ª—ã–µ –Ω–µ–±–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞ (–æ–±—ã—á–Ω–æ 1‚Äì32)
    # - PER: —Ç–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö PER (10,20,11,1,22,320,27 –∏ —Ç.–¥.)
    # - QTY: –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ, –º–Ω–æ–≥–æ –Ω—É–ª–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π > 32

    KNOWN_PER_VALUES = {10, 20, 11, 1, 22, 320, 27}

    pallets_col_idx = None
    per_col_idx = None
    qty_col_idx = None

    # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–µ —Å–ø—Ä–∞–≤–∞
    col_stats = {}
    for idx in right_cols_indices:
        raw = df_data.iloc[:, idx].astype(str).str.replace(",", ".")
        col = pd.to_numeric(raw, errors="coerce")

        non_null = col.dropna()
        if non_null.empty:
            continue

        max_val = non_null.max()
        min_val = non_null.min()
        unique_vals = set(int(v) for v in non_null.unique() if pd.notna(v))

        per_hits = unique_vals.intersection(KNOWN_PER_VALUES)
        zero_share = (col == 0).sum() / len(col)  # –¥–æ–ª—è –Ω—É–ª–µ–π

        col_stats[idx] = {
            "max": max_val,
            "min": min_val,
            "unique": unique_vals,
            "per_hits_count": len(per_hits),
            "zero_share": zero_share,
        }

    # 1) –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–±—Ä–∞—Ç—å PER –ø–æ –Ω–∞–∏–±–æ–ª—å—à–µ–º—É —á–∏—Å–ª—É –ø–æ–ø–∞–¥–∞–Ω–∏–π –≤ KNOWN_PER_VALUES
    if col_stats:
        # –∫–æ–ª–æ–Ω–∫–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º per_hits_count
        per_candidate = max(
            col_stats.items(),
            key=lambda kv: kv[1]["per_hits_count"],
        )
        if per_candidate[1]["per_hits_count"] > 0:
            per_col_idx = per_candidate[0]

    # 2) –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–±—Ä–∞—Ç—å PALLETS —Å—Ä–µ–¥–∏ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è: –Ω–µ–±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è (<= 32)
    for idx, stats in col_stats.items():
        if idx == per_col_idx:
            continue
        if stats["max"] <= 32:
            pallets_col_idx = idx
            break

    # 3) –í—Å—ë, —á—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å, —Å—á–∏—Ç–∞–µ–º QTY (–æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à—Ç—É–∫)
    for idx in right_cols_indices:
        if idx == per_col_idx or idx == pallets_col_idx:
            continue
        if idx in col_stats:
            qty_col_idx = idx
            break

    # --- SANITY CHECK: Pallets vs Qty ---
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Å–º—ã—Å–ª–∞: –∫–æ–ª-–≤–æ –ø–∞–ª–ª–µ—Ç –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–µ–≤—ã—à–∞—Ç—å –∫–æ–ª-–≤–æ —à—Ç—É–∫.
    # –ï—Å–ª–∏ Pallets > Qty, –∑–Ω–∞—á–∏—Ç –∫–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–ø—É—Ç–∞–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qty –º–∞–ª–µ–Ω—å–∫–æ–µ –∏ –ø–æ–ø–∞–ª–æ –ø–æ–¥ —ç–≤—Ä–∏—Å—Ç–∏–∫—É <= 32).
    if pallets_col_idx is not None and qty_col_idx is not None:
        p_vals = pd.to_numeric(
            df_data.iloc[:, pallets_col_idx].astype(str).str.replace(",", "."),
            errors="coerce"
        ).fillna(0)
        q_vals = pd.to_numeric(
            df_data.iloc[:, qty_col_idx].astype(str).str.replace(",", "."),
            errors="coerce"
        ).fillna(0)

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–∞–º, –≥–¥–µ –æ–±–∞ –∑–Ω–∞—á–µ–Ω–∏—è > 0
        mask_check = (p_vals > 0) & (q_vals > 0)
        if mask_check.any():
            violations = (p_vals[mask_check] > q_vals[mask_check]).sum()
            valid_count = mask_check.sum()
            
            # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 50% –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –Ω–∞—Ä—É—à–∞—é—Ç —É—Å–ª–æ–≤–∏–µ -> –º–µ–Ω—è–µ–º –º–µ—Å—Ç–∞–º–∏
            if violations > valid_count * 0.5:
                pallets_col_idx, qty_col_idx = qty_col_idx, pallets_col_idx


    # –ï—Å–ª–∏ PER –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –ø–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º, –Ω–æ –µ—Å—Ç—å 2‚Äì3 –∫–æ–ª–æ–Ω–∫–∏,
    # —Ç–æ –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å –∫—Ä–∞–π–Ω—é—é –ø—Ä–∞–≤—É—é –∫–∞–∫ PER, –µ—Å–ª–∏ —Ç–∞–º –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞.
    if per_col_idx is None and len(right_cols_indices) >= 2:
        candidate = right_cols_indices[-1]
        col = pd.to_numeric(
            df_data.iloc[:, candidate].astype(str).str.replace(",", "."),
            errors="coerce",
        )
        if col.dropna().max() <= 1000:  # –≥—Ä—É–±—ã–π –ª–∏–º–∏—Ç –¥–ª—è PER
            per_col_idx = candidate

    # –¢–µ–ø–µ—Ä—å —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å—ã—Ä—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ARTIKELNR_RAW, QTY_RAW, PALLETS_RAW, PER_RAW
    data = pd.DataFrame()
    data["ARTIKELNR_RAW"] = artikel_col

    # QTY_RAW
    if qty_col_idx is not None:
        data["QTY_RAW"] = df_data.iloc[:, qty_col_idx]
    else:
        data["QTY_RAW"] = ""

    # PALLETS_RAW
    if pallets_col_idx is not None:
        data["PALLETS_RAW"] = df_data.iloc[:, pallets_col_idx]
    else:
        data["PALLETS_RAW"] = ""

    # PER_RAW
    if per_col_idx is not None:
        data["PER_RAW"] = df_data.iloc[:, per_col_idx]
    else:
        data["PER_RAW"] = ""

    # –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ data —Ç–∞–∫–∞—è –∂–µ, –∫–∞–∫ —Ä–∞–Ω—å—à–µ:
    #  ARTIKELNR_RAW, QTY_RAW, PALLETS_RAW, PER_RAW
    # –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∏–∂–µ (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ ORDER_QTY/ORDER_PALLETS)
    # –æ—Å—Ç–∞—ë—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.




    res = pd.DataFrame()
    res["ARTIKELNR"] = data["ARTIKELNR_RAW"].astype(str).str.strip().str.upper()

    res["ORDER_QTY"] = pd.to_numeric(
        data["QTY_RAW"].astype(str).str.replace(",", "."),
        errors="coerce",
    ).fillna(0)

    res["ORDER_PALLETS"] = pd.to_numeric(
        data["PALLETS_RAW"].astype(str).str.replace(",", "."),
        errors="coerce",
    ).fillna(0).astype(int)

    res = res[res["ORDER_PALLETS"] > 0].copy()

    per_vals = pd.to_numeric(
        data["PER_RAW"].astype(str).str.replace(",", "."),
        errors="coerce",
    ).fillna(0)

    missing = (res["ORDER_QTY"] == 0) & (per_vals > 0)
    if missing.any():
        res.loc[missing, "ORDER_QTY"] = (
            res.loc[missing, "ORDER_PALLETS"] * per_vals.loc[missing]
        )

    res["ARTIKELNR"] = res["ARTIKELNR"].astype(str).str.strip()
    res = res[res["ARTIKELNR"] != ""].copy()

    return res

# ---------- Agregacja wielu plik√≥w zam√≥wie≈Ñ ----------

def natural_sort_key(text):
    import re
    parts = re.split(r"(\d+)", str(text).upper())
    return [int(p) if p.isdigit() else p for p in parts]

def extract_date_from_filename(filename):
    """
    Pr√≥buje wyciƒÖgnƒÖƒá datƒô z nazwy pliku.
    Obs≈Çuguje: dd-mm-yyyy, yyyy-mm-dd oraz dd-mm-yy (separatory: - . _)
    """
    s = str(filename)
    
    # 1. Format dd-mm-yyyy (np. 01-05-2023)
    match_dmy = re.search(r"(\d{2})[-._](\d{2})[-._](\d{4})", s)
    if match_dmy:
        d, m, y = match_dmy.groups()
        try:
            return pd.Timestamp(year=int(y), month=int(m), day=int(d)).date()
        except ValueError:
            pass

    # 2. Format yyyy-mm-dd (np. 2023-05-01)
    match_ymd = re.search(r"(\d{4})[-._](\d{2})[-._](\d{2})", s)
    if match_ymd:
        y, m, d = match_ymd.groups()
        try:
            return pd.Timestamp(year=int(y), month=int(m), day=int(d)).date()
        except ValueError:
            pass

    # 3. Format dd-mm-yy (np. 01-05-23) -> zak≈Çada rok 20xx
    match_dmy_short = re.search(r"(\d{2})[-._](\d{2})[-._](\d{2})", s)
    if match_dmy_short:
        d, m, y = match_dmy_short.groups()
        year_full = 2000 + int(y)
        try:
            return pd.Timestamp(year=year_full, month=int(m), day=int(d)).date()
        except ValueError:
            pass
            
    return None

def aggregate_uploaded_orders(uploaded_orders):
    """
    Przyjmuje listƒô plik√≥w ze st.file_uploader,
    zwraca:
      - orders_all: wszystkie wiersze z plik√≥w (ARTIKELNR, ORDER_PALLETS, ORDER_QTY, SOURCE_FILE)
      - orders_agg: agregat po ARTIKELNR z podsumowaniem ilo≈õci
      - valid_count: liczba poprawnie przetworzonych plik√≥w

    Buduje te≈º mapƒô szczeg√≥≈Ç√≥w po artykule: ile sztuk z ka≈ºdego pliku,
    kt√≥ra p√≥≈∫niej jest u≈ºyta do tooltip√≥w w tabeli agregatu.
    """
    # mapa szczeg√≥≈Ç√≥w: ARTIKELNR -> { filename: qty_sum }
    orders_detail_map = {}

    if not uploaded_orders:
        st.session_state["orders_cache"] = {
            "files_keys": None,
            "orders_all": None,
            "orders_agg": None,
            "orders_detail_map": {},
            "valid_count": 0,
        }
        return None, None, 0

    # prosty identyfikator zestawu plik√≥w: nazwy + rozmiar
    files_keys = tuple((getattr(f, "name", ""), getattr(f, "size", None)) for f in uploaded_orders)

    cache = st.session_state.get("orders_cache", {})
    if (
        cache.get("files_keys") == files_keys
        and cache.get("orders_agg") is not None
        and cache.get("orders_detail_map") is not None
        and "valid_count" in cache
        # Sprawdzenie czy cache zawiera kolumnƒô ORDER_DATE (dla kompatybilno≈õci)
        and cache.get("orders_all") is not None and "ORDER_DATE" in cache["orders_all"].columns
    ):
        # u≈ºyj ju≈º policzonych danych ‚Äì bez ponownego parsowania
        return cache["orders_all"], cache["orders_agg"], cache["valid_count"]

    # je≈õli pliki siƒô zmieni≈Çy ‚Äì licz od nowa
    orders_list = []

    for f in uploaded_orders:
        name = getattr(f, "name", "uploaded")
        parsed = parse_order_file_to_df(f)
        if parsed is None:
            continue
        
        if parsed.empty:
            st.warning(f"Plik {name}: nie znaleziono zam√≥wie≈Ñ (pusty wynik).")
            continue

        # dodaj info o ≈∫r√≥dle do wierszy
        parsed = parsed.copy()
        parsed["SOURCE_FILE"] = name
        parsed["ORDER_DATE"] = extract_date_from_filename(name)

        # budowa mapy szczeg√≥≈Ç√≥w: suma sztuk z ka≈ºdego pliku dla danego artyku≈Çu
        grouped = parsed.groupby("ARTIKELNR", as_index=False).agg(
            ORDER_PALLETS=("ORDER_PALLETS", "sum"),
            ORDER_QTY=("ORDER_QTY", "sum"),
        )
        for _, row in grouped.iterrows():
            art = str(row["ARTIKELNR"]).strip().upper()
            qty = float(row["ORDER_QTY"])
            if art not in orders_detail_map:
                orders_detail_map[art] = {}
            orders_detail_map[art][name] = orders_detail_map[art].get(name, 0) + qty

        orders_list.append(parsed)

    if not orders_list:
        st.session_state["orders_cache"] = {
            "files_keys": files_keys,
            "orders_all": None,
            "orders_agg": None,
            "orders_detail_map": {},
            "valid_count": 0,
        }
        return None, None, 0

    # wszystkie wiersze z plik√≥w
    orders_all = pd.concat(orders_list, ignore_index=True)

    # agregat po ARTIKELNR (tylko z plik√≥w, bez rƒôcznych)
    orders_agg = orders_all.groupby("ARTIKELNR", as_index=False).agg(
        ORDER_PALLETS=("ORDER_PALLETS", "sum"),
        ORDER_QTY=("ORDER_QTY", "sum"),
    )

    # tylko artyku≈Çy z paletami > 0
    orders_agg = orders_agg[orders_agg["ORDER_PALLETS"] > 0].copy()

    # naturalna sortowanie po ARTIKELNR
    orders_agg["_sort_key"] = orders_agg["ARTIKELNR"].apply(natural_sort_key)
    orders_agg = orders_agg.sort_values("_sort_key").drop(columns=["_sort_key"]).reset_index(drop=True)

    valid_count = len(orders_list)

    # zapisz do cache
    st.session_state["orders_cache"] = {
        "files_keys": files_keys,
        "orders_all": orders_all,
        "orders_agg": orders_agg,
        "orders_detail_map": orders_detail_map,
        "valid_count": valid_count,
    }

    return orders_all, orders_agg, valid_count

def make_order_tooltip(art, orders_detail_map, manual_agg):
    lines = []
    a = str(art).strip().upper()

    if a in orders_detail_map:
        for fname, qty in orders_detail_map[a].items():
            if qty != 0:
                lines.append(f"{fname} - {int(qty)} szt.")

    if manual_agg is not None and not manual_agg.empty:
        man_row = manual_agg[manual_agg["ARTIKELNR"] == a]
        if not man_row.empty:
            mq = float(man_row["Manual_Qty"].iloc[0])
            if mq != 0:
                lines.append(f"Dodatkowe zam√≥wienia - {int(mq)} szt.")

    if not lines:
        return "Brak informacji z plik√≥w zam√≥wie≈Ñ"

    return " ; ".join(lines)



# ---------- Rƒôczne zam√≥wienia ‚Äì –±—ã—Å—Ç—Ä—ã–π –¥–æ–∑–∞–∫–∞–∑ –±–µ–∑ —Ç–∞–±–ª–∏—Ü—ã ----------

def init_manual_orders():
    """
    Bufor edytora (manual_orders_editor_df) zawsze ma przynajmniej jeden pusty wiersz.
    Committed ‚Äì to ju≈º dodane do agregatu zam√≥wie≈Ñ.
    """
    if "manual_orders_editor_df" not in st.session_state:
        st.session_state.manual_orders_editor_df = pd.DataFrame(
            {"ARTIKELNR": [""], "ORDER_PALLETS": [0], "ORDER_QTY": [0]}
        )
    if "manual_orders_committed_df" not in st.session_state:
        st.session_state.manual_orders_committed_df = pd.DataFrame(
            {"ARTIKELNR": [], "ORDER_PALLETS": [], "ORDER_QTY": []}
        )

def render_manual_orders_editor(artikel_options):
    """
    Prosty formularz do rƒôcznych zam√≥wie≈Ñ:
      - wprowadzanie jednej pozycji na raz,
      - bufor jest niewidoczny ‚Äì od razu dodajemy do agregatu,
      - lista ju≈º dodanych rƒôcznych zam√≥wie≈Ñ na dole.
    """
    init_manual_orders()

    if "manual_order_msg" in st.session_state:
        st.success(st.session_state.pop("manual_order_msg"))

    st.subheader(STR["manual_orders"])

    # 1) Formularz jednej pozycji
    st.markdown("#### Dodaj pojedynczy artyku≈Ç do rƒôcznych zam√≥wie≈Ñ")

    col_a, col_p, col_q, col_btn = st.columns([3, 1, 1, 1])

    with col_a:
        options = [""] + artikel_options
        new_art = st.selectbox(
            "ARTIKELNR",
            options=options,
            index=0,
            key="manual_artikel_select",
        )

    with col_p:
        new_pallets = st.number_input(
            "Palety",
            min_value=0,
            value=0,
            key="manual_pallets_input",
        )

    with col_q:
        new_qty = st.number_input(
            "Ilo≈õƒá sztuk",
            min_value=0,
            value=0,
            key="manual_qty_input",
        )

    with col_btn:
        st.write("")
        if st.button("Dodaj wiersz", key="manual_add_row_btn"):
            # 1) –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ç–∏–∫—É–ª–∞
            if not new_art or not new_art.strip():
                st.warning("Wybierz ARTIKELNR przed dodaniem.")
            else:
                art_norm = new_art.strip().upper()

                # Pozwalamy na wpisanie rƒôczne artyku≈Çu spoza filtr√≥w:
                # je≈õli nie ma go w artikel_options, tylko ostrzegamy.
                if art_norm not in [a.strip().upper() for a in artikel_options]:
                    st.warning("Ten ARTIKELNR nie jest na li≈õcie filtrowanej, ale zostanie dodany rƒôcznie.")

                # 2) –ü—Ä–æ–≤–µ—Ä–∫–∞ ilo≈õci
                if int(new_pallets) == 0 and int(new_qty) == 0:
                    st.warning("Podaj liczbƒô palet lub ilo≈õƒá sztuk przed dodaniem wiersza.")
                else:
                    # 3) Od razu dodajemy do manual_orders_committed_df
                    new_row = pd.DataFrame(
                        {
                            "ARTIKELNR": [art_norm],
                            "ORDER_PALLETS": [int(new_pallets)],
                            "ORDER_QTY": [int(new_qty)],
                        }
                    )

                    st.session_state.manual_orders_committed_df = pd.concat(
                        [st.session_state.manual_orders_committed_df, new_row],
                        ignore_index=True,
                    )

                    st.success(f"Dodano artyku≈Ç {art_norm} do rƒôcznych zam√≥wie≈Ñ.")


    st.markdown("---")

    # 2) –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Å–µ—Ö —Ä—É—á–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤ (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    if st.button("üóë Usu≈Ñ wszystkie rƒôczne zam√≥wienia", type="secondary", key="clear_manual_committed"):
        st.session_state.manual_orders_committed_df = pd.DataFrame(
            {"ARTIKELNR": [], "ORDER_PALLETS": [], "ORDER_QTY": []}
        )
        st.success("Wyczyszczono wszystkie rƒôczne zam√≥wienia.")

    st.markdown("#### Rƒôczne zam√≥wienia dodane do agregatu")

    committed = st.session_state.manual_orders_committed_df

    if not committed.empty:
        committed_display = committed.copy()
        committed_display["ARTIKELNR"] = committed_display["ARTIKELNR"].astype(str).str.strip().str.upper()
        committed_display["ORDER_PALLETS"] = pd.to_numeric(
            committed_display["ORDER_PALLETS"], errors="coerce"
        ).fillna(0).astype(int)
        committed_display["ORDER_QTY"] = pd.to_numeric(
            committed_display["ORDER_QTY"], errors="coerce"
        ).fillna(0)

        # –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –≤—ã–±–æ—Ä–æ–º –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        committed_display["USUN"] = False

        edited = st.data_editor(
            committed_display,
            width="stretch",
            hide_index=True,
            key="manual_committed_editor",
            column_config={
                "ARTIKELNR": st.column_config.TextColumn("ARTIKELNR", disabled=True),
                "ORDER_PALLETS": st.column_config.NumberColumn("Palety", disabled=True),
                "ORDER_QTY": st.column_config.NumberColumn("Ilo≈õƒá sztuk", disabled=True),
                "USUN": st.column_config.CheckboxColumn("Usu≈Ñ"),
            },
        )

        col_del_one, col_space = st.columns([1, 3])
        with col_del_one:
            def delete_selected_callback():
                # Pobieramy zmiany bezpo≈õrednio ze stanu edytora
                editor_state = st.session_state.get("manual_committed_editor", {})
                edited_rows = editor_state.get("edited_rows", {})
                indices_to_remove = [int(k) for k, v in edited_rows.items() if v.get("USUN") is True]
                
                if indices_to_remove:
                    df = st.session_state.manual_orders_committed_df
                    # Filtrujemy indeksy, aby uniknƒÖƒá b≈Çƒôd√≥w
                    valid_indices = [i for i in indices_to_remove if i in df.index]
                    if valid_indices:
                        st.session_state.manual_orders_committed_df = df.drop(valid_indices).reset_index(drop=True)
                        st.session_state["manual_order_msg"] = "Usuniƒôto zaznaczone wiersze z rƒôcznych zam√≥wie≈Ñ."
            
            st.button("üóë Usu≈Ñ zaznaczone wiersze", key="manual_delete_selected_committed", on_click=delete_selected_callback)
    else:
        st.info("Brak rƒôcznych zam√≥wie≈Ñ w agregacie.")



# ---------- G≈Ç√≥wna funkcja zak≈Çadki 'Zam√≥wienia' ----------

def render_orders_tab(artikel_options, filtered_pallets_df=None, selected_artikel=None, filtered_pallets_no_art_df=None, full_df=None, date_start=None, date_end=None, selected_mandant=None):
    """
    G≈Ç√≥wna funkcja dla analizy palet + zam√≥wie≈Ñ.
    """
    from utils import load_excluded_articles  # ‚Üê –¢–û–õ–¨–ö–û 4 –ø—Ä–æ–±–µ–ª–∞!

    
    # 1) –ü–ï–†–í–´–ô –ë–õ–û–ö: –¢–∞–±–ª–∏—Ü–∞ –ø–∞–ª–ª–µ—Ç + –∏—Ö —Å—É–º–º–∞ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É
    st.subheader("üìã Lista palet")
    
    if filtered_pallets_df is not None and not filtered_pallets_df.empty:
        cols_show = [
            "ARTIKELNR",
            "ARTBEZ1",
            "QUANTITY",
            "LHMNR",
            "ZUSTAND",
            "PLATZ",
            "IN_DATE",
            "IN_TIME",
            "OUT_DATE",
            "OUT_TIME",
        ]

        df_show = filtered_pallets_df[cols_show].sort_values(by="OUT_DATE", ascending=False).reset_index(drop=True)
        
        # Formatowanie daty do YYYY-MM-DD (bez godziny)
        df_show["IN_DATE"] = df_show["IN_DATE"].dt.date
        df_show["OUT_DATE"] = df_show["OUT_DATE"].dt.date

        st.dataframe(df_show, width="stretch", hide_index=True)
        
        # Agregacja widocznych palet (podsumowanie)
        st.markdown("#### ‚àë Podsumowanie listy palet")
        df_list_agg = filtered_pallets_df.groupby(["ARTIKELNR", "ARTBEZ1"], as_index=False).agg(
            Liczba_palet=("LHMNR", "nunique"),
            Suma_sztuk=("QUANTITY", "sum")
        ).rename(columns={"Liczba_palet": "Liczba palet", "Suma_sztuk": "Suma sztuk"}).sort_values("Liczba palet", ascending=False)
        st.dataframe(df_list_agg, width="stretch", hide_index=True)

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ –¥–Ω—è–º (–≤ expanders)
        with st.expander("üìä Szczeg√≥≈Çy przyjƒôƒá i usuniƒôƒá wed≈Çug dnia", expanded=False):
            if not selected_artikel:
                st.info("Wybierz artyku≈Ç w filtrach, aby zobaczyƒá szczeg√≥≈ÇowƒÖ tabelƒô po dniach.")
            elif full_df is not None and selected_mandant and date_start and date_end:
                # --- Przygotowanie danych niezale≈ºnie od trybu (Wej≈õcie/Wyj≈õcie) ---
                
                # 1. Filtr Mandant i Artyku≈Ç
                mask_base = (full_df["MANDANT"].astype(str) == str(selected_mandant))
                mask_base &= full_df["ARTIKELNR"].isin([a.strip().upper() for a in selected_artikel])
                df_subset = full_df[mask_base]

                # 2. Przyjƒôcia (IN_DATE w zakresie dat)
                mask_in = df_subset["IN_DATE"].between(pd.Timestamp(date_start), pd.Timestamp(date_end))
                df_in = df_subset[mask_in].copy()

                if not df_in.empty:
                    daily_accepted = df_in.groupby(["ARTIKELNR", "IN_DATE"], as_index=False).agg(
                        Palety_przyjƒôte=("LHMNR", "nunique"),
                        Sztuki_przyjƒôte=("QUANTITY", "sum")
                    )
                    daily_accepted["IN_DATE"] = daily_accepted["IN_DATE"].dt.date
                    daily_accepted = daily_accepted.sort_values(["ARTIKELNR", "IN_DATE"], ascending=[True, False])
                    
                    st.subheader("üì• Przyjƒôcia wed≈Çug dnia")
                    st.dataframe(daily_accepted, width="stretch", hide_index=True)
                else:
                    st.info("Brak przyjƒôtych palet dla wybranego artyku≈Çu w wybranym zakresie dat.")

                st.markdown("---")

                # 3. Usuniƒôcia (OUT_DATE w zakresie dat + IS_DELETED)
                mask_out = df_subset["OUT_DATE"].between(pd.Timestamp(date_start), pd.Timestamp(date_end))
                if "IS_DELETED" in df_subset.columns:
                    mask_deleted = df_subset["IS_DELETED"]
                else:
                    mask_deleted = df_subset["ZUSTAND"] != "401"
                
                df_out = df_subset[mask_out & mask_deleted].copy()

                if not df_out.empty:
                    daily_deleted = df_out.groupby(["ARTIKELNR", "OUT_DATE"], as_index=False).agg(
                        Palety_usuniƒôte=("LHMNR", "nunique"),
                        Sztuki_usuniƒôte=("QUANTITY", "sum")
                    )
                    daily_deleted["OUT_DATE"] = daily_deleted["OUT_DATE"].dt.date
                    daily_deleted = daily_deleted.sort_values(["ARTIKELNR", "OUT_DATE"], ascending=[True, False])
                    
                    st.subheader("üóëÔ∏è Usuniƒôcia wed≈Çug dnia")
                    st.dataframe(daily_deleted, width="stretch", hide_index=True)
                else:
                    st.info("Brak usuniƒôtych palet dla wybranego artyku≈Çu w wybranym zakresie dat.")
            else:
                st.warning("Brak danych do analizy szczeg√≥≈Çowej.")


    else:
        st.info("Brak palet w wybranym zakresie filtr√≥w.")


    st.markdown("---")

    # 2) –í–¢–û–†–û–ô –ë–õ–û–ö: Zam√≥wienia (pliki + rƒôczne)
    st.subheader("üì¶ Zam√≥wienia")

    if "orders_uploader_key" not in st.session_state:
        st.session_state["orders_uploader_key"] = 0

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –∑–∞–∫–∞–∑–æ–≤
    uploaded_orders = st.file_uploader(
        STR["upload_orders"],
        type=["xlsx", "csv", "txt"],
        accept_multiple_files=True,
        key=f"orders_uploader_{st.session_state['orders_uploader_key']}",
    )

    if uploaded_orders:
        if st.button("üóëÔ∏è Usu≈Ñ wszystkie pliki zam√≥wie≈Ñ", key="clear_all_orders_btn"):
            st.session_state["orders_cache"] = {
                "files_keys": None,
                "orders_all": None,
                "orders_agg": None,
                "orders_detail_map": {},
                "valid_count": 0,
            }
            st.session_state["orders_uploader_key"] += 1
            st.rerun()

    orders_all, orders_agg_base, valid_files_count = aggregate_uploaded_orders(uploaded_orders)

    if uploaded_orders:
        st.caption(f"Za≈Çadowano plik√≥w: {len(uploaded_orders)} | Poprawnie odczytano: {valid_files_count}")

    # Rƒôczne zam√≥wienia
    render_manual_orders_editor(artikel_options)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤
    manual_df = st.session_state.get("manual_orders_committed_df", pd.DataFrame(
        {"ARTIKELNR": [], "ORDER_PALLETS": [], "ORDER_QTY": []}
    ))

    if orders_agg_base is None and manual_df.empty:
        st.info("Brak danych z plik√≥w zam√≥wie≈Ñ ani z rƒôcznych zam√≥wie≈Ñ.")
        return

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ + rƒôczne
    manual_agg = None
    if not manual_df.empty:
        m = manual_df.copy()
        m["ARTIKELNR"] = m["ARTIKELNR"].astype(str).str.strip().str.upper()
        m["ORDER_PALLETS"] = pd.to_numeric(m["ORDER_PALLETS"], errors="coerce").fillna(0).astype(int)
        m["ORDER_QTY"] = pd.to_numeric(m["ORDER_QTY"], errors="coerce").fillna(0)

        manual_agg = m.groupby("ARTIKELNR", as_index=False).agg(
            Manual_Pallets=("ORDER_PALLETS", "sum"),
            Manual_Qty=("ORDER_QTY", "sum"),
        )

    # Finalny agregat
    if orders_agg_base is not None:
        orders_agg = orders_agg_base.copy()
    else:
        orders_agg = pd.DataFrame(columns=["ARTIKELNR", "ORDER_PALLETS", "ORDER_QTY"])

    if manual_agg is not None and not manual_agg.empty:
        orders_agg = orders_agg.merge(manual_agg, on="ARTIKELNR", how="outer")
    else:
        orders_agg["Manual_Pallets"] = 0
        orders_agg["Manual_Qty"] = 0

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    for col in ["ORDER_PALLETS", "Manual_Pallets"]:
        orders_agg[col] = pd.to_numeric(orders_agg[col], errors="coerce").fillna(0).astype(int)
    for col in ["ORDER_QTY", "Manual_Qty"]:
        orders_agg[col] = pd.to_numeric(orders_agg[col], errors="coerce").fillna(0)

    orders_agg["Ordered_Pallets_Total"] = orders_agg["ORDER_PALLETS"] + orders_agg["Manual_Pallets"]
    orders_agg["Ordered_Qty_Total"] = orders_agg["ORDER_QTY"] + orders_agg["Manual_Qty"]

    # ≈πr√≥d≈Ça
    cache = st.session_state.get("orders_cache", {})
    orders_detail_map = cache.get("orders_detail_map", {})

    def sources_count(row):
        art = str(row["ARTIKELNR"]).strip().upper()
        files_sources = sum(1 for _, qty in orders_detail_map.get(art, {}).items() if qty != 0)
        manual_source = 1 if row.get("Manual_Qty", 0) > 0 else 0
        return files_sources + manual_source

    def is_excluded_article(art, excluded_exact, excluded_prefixes):
        art = str(art).strip().upper()
        if art in [e.upper() for e in excluded_exact]:
            return True
        for p in excluded_prefixes:
            if art.startswith(p.upper()):
                return True
        return False

    orders_agg["≈πr√≥d≈Ça"] = orders_agg.apply(sources_count, axis=1)
    orders_agg["ORDER_TOOLTIP"] = orders_agg["ARTIKELNR"].apply(
        lambda a: make_order_tooltip(a, orders_detail_map, manual_agg)
    )

    # –¢–∞–±–ª–∏—Ü–∞ –∑–∞–∫–∞–∑–æ–≤
    st.subheader("üìã Podsumowanie zam√≥wie≈Ñ (agregat)")
    display_cols = ["ARTIKELNR", "Ordered_Pallets_Total", "Ordered_Qty_Total", "≈πr√≥d≈Ça", "ORDER_TOOLTIP"]
    display_df = orders_agg[display_cols].copy()
    display_df.rename(columns={
        "ARTIKELNR": "ARTIKELNR",
        "Ordered_Pallets_Total": "Zam√≥wione_palety",
        "Ordered_Qty_Total": "Zam√≥wione_sztuki",
        "ORDER_TOOLTIP": "Szczeg√≥≈Çy_≈∫r√≥de≈Ç",
    }, inplace=True)

    st.dataframe(
        display_df,
        width="stretch",
        hide_index=True,
    )

    # 3) –°–†–ê–í–ù–ï–ù–ò–ï (–µ—Å–ª–∏ –µ—Å—Ç—å –ø–∞–ª–µ—Ç—ã)
    # U≈ºywamy danych bez filtra artyku≈Ç√≥w (je≈õli dostƒôpne), aby metryki por√≥wnawcze by≈Çy globalne
    df_for_comparison = filtered_pallets_no_art_df if filtered_pallets_no_art_df is not None else filtered_pallets_df

    if df_for_comparison is not None and not df_for_comparison.empty:
        st.markdown("---")
        st.subheader("‚öñÔ∏è Por√≥wnanie zam√≥wie≈Ñ z usuniƒôtymi paletami")

        deleted_pallets = df_for_comparison[df_for_comparison["IS_DELETED"]].copy()

        if not deleted_pallets.empty:
            deleted_agg = deleted_pallets.groupby("ARTIKELNR", as_index=False).agg(
                Deleted_Pallets=("LHMNR", "nunique"),
                Deleted_Qty=("QUANTITY", "sum"),
            )

            comparison_df = orders_agg[["ARTIKELNR", "Ordered_Pallets_Total", "Ordered_Qty_Total"]].merge(
                deleted_agg, on="ARTIKELNR", how="outer"
            ).fillna(0)

            comparison_df["R√≥≈ºnica_Palety"] = (
                comparison_df["Ordered_Pallets_Total"] - comparison_df["Deleted_Pallets"]
            )
            comparison_df["R√≥≈ºnica_Sztuki"] = (
                comparison_df["Ordered_Qty_Total"] - comparison_df["Deleted_Qty"]
            )

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ –±–µ–∑ —Ä–∞–∑–Ω–∏—Ü—ã
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
            excluded_exact, excluded_prefixes = load_excluded_articles()
            
            def should_show_row(row):
                art = row["ARTIKELNR"].strip().upper()
                if is_excluded_article(art, excluded_exact, excluded_prefixes):
                    # –ò—Å–∫–ª—é—á–µ–Ω–∏—è: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –û–ë–ï —Ä–∞–∑–Ω–∏—Ü—ã –ù–ï –Ω–æ–ª—å
                    return (row["R√≥≈ºnica_Palety"] != 0) and (row["R√≥≈ºnica_Sztuki"] != 0)
                else:
                    # –û–±—ã—á–Ω—ã–µ: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –µ—Å–ª–∏ –•–û–¢–¨ –û–î–ù–ê —Ä–∞–∑–Ω–∏—Ü–∞
                    return (row["R√≥≈ºnica_Palety"] != 0) or (row["R√≥≈ºnica_Sztuki"] != 0)
            
            comparison_df = comparison_df[comparison_df.apply(should_show_row, axis=1)]


            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º
            def explain_diff(row):
                diff_pal = row["R√≥≈ºnica_Palety"]
                diff_szt = row["R√≥≈ºnica_Sztuki"]

                if diff_pal == 0 and diff_szt == 0:
                    return "Brak r√≥≈ºnicy"
                msgs = []

                if diff_pal > 0:
                    msgs.append(f"Usuniƒôto {int(abs(diff_pal))} palet mniej")
                elif diff_pal < 0:
                    msgs.append(f"Usuniƒôto {int(abs(diff_pal))} palet wiƒôcej")
                else:
                    msgs.append("Brak r√≥≈ºnicy w liczbie palet")

                if diff_szt > 0:
                    msgs.append(f"zabrak≈Ço {int(abs(diff_szt))} sztuk")
                elif diff_szt < 0:
                    msgs.append(f"jest {int(abs(diff_szt))} sztuk za du≈ºo")
                else:
                    msgs.append("brak r√≥≈ºnicy w ilo≈õci sztuk")

                return ", ".join(msgs)

            comparison_df["Wyja≈õnienie r√≥≈ºnicy"] = comparison_df.apply(explain_diff, axis=1)

            comparison_df = comparison_df.sort_values("R√≥≈ºnica_Palety", ascending=False).reset_index(drop=True)

            # --- Analiza dzienna (Daily Breakdown) ---
            # Pokazujemy kolumnƒô tylko je≈õli wybrano zakres dat (> 1 dzie≈Ñ)
            is_date_range = date_start and date_end and (date_end.date() - date_start.date()).days > 0

            if is_date_range and orders_all is not None and "ORDER_DATE" in orders_all.columns and not orders_all.empty:
                # 1. Zam√≥wienia wg daty
                orders_valid = orders_all.dropna(subset=["ORDER_DATE"]).copy()
                
                # Ostrze≈ºenie o plikach bez daty
                missing_date_mask = orders_all["ORDER_DATE"].isna()
                if missing_date_mask.any():
                    missing_files = orders_all.loc[missing_date_mask, "SOURCE_FILE"].unique()
                    if len(missing_files) > 0:
                        st.warning(
                            f"‚ö†Ô∏è Uwaga: Nie rozpoznano daty w nazwach {len(missing_files)} plik√≥w (np. {missing_files[0]}). "
                            "Zam√≥wienia z tych plik√≥w sƒÖ wliczone w sumƒô og√≥lnƒÖ, ale NIE pojawiƒÖ siƒô w kolumnie 'Dni z r√≥≈ºnicƒÖ'."
                        )

                if not orders_valid.empty:
                    orders_daily = orders_valid.groupby(["ARTIKELNR", "ORDER_DATE"], as_index=False)["ORDER_PALLETS"].sum()
                    orders_daily.rename(columns={"ORDER_DATE": "DATE", "ORDER_PALLETS": "ORD"}, inplace=True)
                else:
                    orders_daily = pd.DataFrame(columns=["ARTIKELNR", "DATE", "ORD"])
                
                # 2. Usuniƒôcia wg daty (z deleted_pallets)
                if not deleted_pallets.empty:
                    del_daily = deleted_pallets.copy()
                    del_daily["DATE"] = del_daily["OUT_DATE"].dt.date
                    del_daily_agg = del_daily.groupby(["ARTIKELNR", "DATE"], as_index=False)["LHMNR"].nunique()
                    del_daily_agg.rename(columns={"LHMNR": "DEL"}, inplace=True)
                else:
                    del_daily_agg = pd.DataFrame(columns=["ARTIKELNR", "DATE", "DEL"])

                # 3. ≈ÅƒÖczenie i obliczanie r√≥≈ºnic
                if not orders_daily.empty or not del_daily_agg.empty:
                    daily_merged = pd.merge(orders_daily, del_daily_agg, on=["ARTIKELNR", "DATE"], how="outer").fillna(0)
                    daily_merged["DIFF"] = daily_merged["ORD"] - daily_merged["DEL"]
                    
                    # Filtrowanie tylko r√≥≈ºnic
                    daily_diffs = daily_merged[daily_merged["DIFF"] != 0].copy()
                    
                    if not daily_diffs.empty:
                        daily_diffs = daily_diffs.sort_values("DATE")
                        
                        def fmt_diff(row):
                            d_str = row["DATE"].strftime("%d.%m")
                            val = int(row["DIFF"])
                            sign = "+" if val > 0 else ""
                            return f"{d_str}: {sign}{val}"

                        daily_diffs["TXT"] = daily_diffs.apply(fmt_diff, axis=1)
                        
                        daily_map = daily_diffs.groupby("ARTIKELNR")["TXT"].apply(lambda x: "\n".join(x)).to_dict()
                        
                        comparison_df["Dni z r√≥≈ºnicƒÖ"] = comparison_df["ARTIKELNR"].map(daily_map).fillna("-")
                    else:
                        comparison_df["Dni z r√≥≈ºnicƒÖ"] = "-"
                else:
                    comparison_df["Dni z r√≥≈ºnicƒÖ"] = "-"
            elif is_date_range:
                comparison_df["Dni z r√≥≈ºnicƒÖ"] = "-"

            st.dataframe(
                comparison_df,
                width="stretch",
                hide_index=True,
            )

            # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            col1, col2, col3 = st.columns(3)
            col1.metric("Artyku≈Çy z zam√≥wieniami", f"{len(orders_agg[orders_agg['Ordered_Pallets_Total'] > 0])}")
            col2.metric("Artyku≈Çy usuniƒôte", f"{len(deleted_agg)}")
            col3.metric("Artyku≈Çy z rozbie≈ºno≈õciƒÖ", f"{len(comparison_df)}")
        else:
            st.info("Brak usuniƒôtych palet w wybranym zakresie.")
